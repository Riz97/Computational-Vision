{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: pink; padding: 10px\">\n",
    "# YOUR SURNAME: ______________ YOUR NAME: _________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Viola and Jones face detector\n",
    "\n",
    "The goal of the lab is to encourage you do dive deeper into some core aspects of the VJ detector implementation and usage. The activity is devided into 2 somewhat independent parts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: pink; padding: 10px\">\n",
    "Read all the parts (text and comments), and add your own comments whenever you find them appropriate (on the implementation choices, on the output you obtain...), but in particular where you are asked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Understanding building blocks\n",
    "\n",
    "### 1.1: Integral images\n",
    "\n",
    "Even if you will find an implementation of integral images already available on OpenCV, it will be instructive to do your own. Follow the instructions below.\n",
    "\n",
    "<i>NOTICE (This was not clearly explained in the class but it is important to obtain an efficient implementation):</i>\n",
    "<br>\n",
    "    In each position of the image <tt>(y,x)</tt> except from the first row (y=0) and first column (x=0) the value of the value of the integral image is computed by updating previously computed values see as follows  \n",
    "<img src=\"sketch.png\" width=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integral_image(image):\n",
    "    height, width = image.shape\n",
    "    ii = np.zeros(image.shape)\n",
    "    s = np.zeros(image.shape) # aux structure to precompute the sum of preceeding elements in the current row\n",
    "    for y in range(height): \n",
    "        for x in range(width): \n",
    "            s[y][x] = s[y-1][x] + image[y][x] if y > 0 else image[y][x]\n",
    "            ii[y][x] = # FILL HERE\n",
    "    return ii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test</b> it on a real image and on a synthetic image.\n",
    "To <b>evaluate</b> the quality of your result you may compare it with the OpenCV implementation <tt>cv2.integral</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a real image\n",
    "img = cv2.imread('Lena.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(integral_image(gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdfc7a87b90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BUILD a simple synthetic B/W image, for instance white rectangles over a black background (but you can make different ones)\n",
    "\n",
    "# here's my example\n",
    "\n",
    "I = np.zeros((100,100))\n",
    "\n",
    "I[0:20,0:10]=255\n",
    "I[50:60,50:60]=255\n",
    "\n",
    "# before you run the integral image function ask yourself how you would expect the integral image to be in this case. \n",
    "# in the next cell you will check the actual output and be sure it makes sense to you\n",
    "\n",
    "plt.imshow(I,cmap='gray')      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now you can visualize the integral image \n",
    "plt.subplot(1,2,1) \n",
    "plt.imshow(I,cmap='gray')    \n",
    "plt.subplot(1,2,2)\n",
    "ii=integral_image(I)\n",
    "plt.imshow(ii,cmap='gray')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: pink; padding: 10px\">\n",
    "On this first part there are no comments to add, but I suggest you do your best to check that  your implementation is correct and that at this point you really understand what an integral image is (you may try and change the synthetic image and see if the ii you produce makes sense to you)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: The response of a rectangle feature of your choice\n",
    "\n",
    "We now reason on the quality and quantity of rectangle features over an image. You may use the synthetic image for this part.\n",
    "\n",
    "You first need to implement a function <tt>compute_rectangle</tt> that computes the sum of all the elements within a rectangle, exploiting the integral image (see formula on the slides)\n",
    "\n",
    "Then \n",
    "- Select a Haar feature of your choice, choosing the pattern, the size and the aspect ratio. The only parameter changing would be its position wrt the image\n",
    "- Apply it to the image and obtain a feature vector. Does the size and its values make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii is an integral image, x starting column, y starting row, width and height of the rectangle\n",
    "# search the formula in the slides\n",
    "def compute_rectangle(ii, x, y, width, height):\n",
    "    return # FILL HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a very naif implementation, you should just use it to get a feeling \n",
    "# of the procedure. Try and understand what it does\n",
    "# if you've time you can do better than this\n",
    "# here I chose a feature of a 20x20 square feature with a vertical WHITE,BLACK structure (fixed scale and aspect ratio)\n",
    "\n",
    "def build_my_features(ii):\n",
    "    height, width = ii.shape\n",
    "    myw=10\n",
    "    myh=20\n",
    "    m=5 # feature's min size\n",
    "    features = []\n",
    "    #for w in range(m, width-m):  # you should uncomment this if you want to try out different sizes and aspect ratio\n",
    "        #for h in range(m, height-m):\n",
    "    for w in range(myw, myw+1): #these are dummy for just for consistency with the previous commented ones!\n",
    "        for h in range(myh, myh+1):\n",
    "            i = 0\n",
    "            while i + 2*w < width:\n",
    "                j = 0\n",
    "                while j + h < height:\n",
    "                    pos = compute_rectangle(ii,i, j, w, h)  \n",
    "                    neg = compute_rectangle(ii,i+w, j, w, h)  \n",
    "                    features.append(([pos],[neg] ))              \n",
    "                    j += 1\n",
    "                i += 1\n",
    "    return features\n",
    "\n",
    "# features is a list of [[pos] [neg]] elements, one per each appropriate position in the image\n",
    "# pos contains the sum of pixels values corresponding to the feature white area (of a given position in the image)\n",
    "# neg contains the sum of pixels values corresponding to the feature black area "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: pink; padding: 10px\">  <b>OPTIONAL</b>: implement another function that computes 20x20 square features with a <b> horizontal </b> WHITE,BLACK structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this function you will explore the list of features previosuly computed and compute the feature value \n",
    "#  (sum white elements - sum black elements)\n",
    "def apply_features(features):\n",
    "    X = np.zeros((len(features)))\n",
    "    i=0\n",
    "    for positive_region, negative_region in features:\n",
    "        X[i] = positive_region[0]-negative_region[0]\n",
    "        i += 1\n",
    "    return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the functions on the synthetic image and\n",
    "# for a better understanding you should change the synthetic image you're using\n",
    "F=build_my_features(ii)\n",
    "A=apply_features(F)\n",
    "plt.plot(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: pink; padding: 10px\">\n",
    "Describe the output you obtain, does it make sense? to you understand what it means? (it may be useful to change the input synthetic feature to see how the feature vector changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just computed a feature vector of an image for a subset of the possible rectangle features (1 type, 1 aspect ratio, 1 size). Optionally, I suggested you could try with another one, in case you tried, the two vectors should be concatenated. \n",
    "\n",
    "In the real case we would concatenate all the possible variants of feature type, aspect ratio, size. Then we would build a feature matrix with one row per image. Similar image should have the same structure.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Give it a try!\n",
    "\n",
    "### 2.2 Face detection \n",
    "\n",
    "By now you should have a feeling of the type of feature vectors we may compute. Feature selection and training would be too long for a 2 hours lab, thus we play with the pre-implemented cascade classifier. You'll need to get familiar with the OpenCV function\n",
    "\n",
    "<tt> Python: cv2.CascadeClassifier.detectMultiScale(image, scale_factor=1.1, min_neighbors=3, flags=0, min_size=(0, 0)) → detectedObjects</tt>\n",
    "\n",
    "Parameters:\t\n",
    "- image – Matrix containing a graylevel image where objects are detected.\n",
    "- objects – Vector of rectangles where each rectangle contains the detected object.\n",
    "- scaleFactor – Parameter specifying how much the image size is reduced at each image scale.\n",
    "- minNeighbors – Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
    "- flags – Parameter with the same meaning for an old cascade as in the function cvHaarDetectObjects. It is not used for a new cascade.\n",
    "- minSize – Minimum possible object size. Objects smaller than that are ignored.\n",
    "- maxSize – Maximum possible object size. Objects larger than that are ignored.\n",
    "\n",
    "The detected objects will be returned as a list of rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load a classifier from an XML file\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# We now load an image and turn it to gray levels (don't forget this!)\n",
    "img = cv2.imread('Lena.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(gray,cmap='gray' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice: the following call is correct, but you must improve the results by adding optional parameters \n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# let's have a look at the output\n",
    "print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rects(img, rects, color):\n",
    "    for x1, y1, x2, y2 in rects:\n",
    "        cv2.rectangle(img, (x1, y1), (x1+x2, y1+y2), color, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_rects(#FILL HERE THE FUNCTION PARAMETERS\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect and visualize faces in the following two images, complete all the required steps\n",
    "\n",
    "cdp = cv2.imread('cdp2.jpg')\n",
    "\n",
    "# FILL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got = cv2.imread('got.png')\n",
    "\n",
    "# FILL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model we are using is not robust to inplane rotations, you should check this limitation by comparing \n",
    "# the outputs obtained by the following two images \n",
    "tr = cv2.imread('truman1.jpg')\n",
    "\n",
    "# FILL HERE\n",
    "\n",
    "tr2 = cv2.imread('truman2.jpg')\n",
    "\n",
    "# FILL HERE\n",
    "\n",
    "# If you apply an appropriate rotation to the second image, results should improve, why?\n",
    "\n",
    "# FILL HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: pink; padding: 10px\">\n",
    "The previous tests, involved different types of images and produced different results. Try you reach some conclusion on the limits of the algorithm and, if you want, make an attempt in explaining how they could be mitigated.\n",
    "\n",
    "It may be interesting to try out other images of your choice (e.g. lateral faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 (Optional)  Nested classifiers\n",
    "\n",
    "The same procedure may be used to train other objects / object parts detectors. Here we will consider an <i> eye detector </i> and will call it in a nested modality (that is inside a region where a face was detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "img = cv2.imread('Lena.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(gray,cmap='gray' )\n",
    "\n",
    "faces = # FILL HERE (DETECT FACES as we did before)\n",
    "for (x,y,w,h) in faces:\n",
    "    draw_rects(img,faces,(0,255,0))\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex +ew,ey+eh),(0,0,255),2) #we are exploiting aliases between matrices...\n",
    "\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 (Optional) put a face / eye detector in the webcam loop\n",
    "\n",
    "The following function may be used to run the face (or face+eye) detector on a live video stream acquired through your cam. Try out different acquisition conditions, try and understand the potential and the limits of this powerfull but simple detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_webcam(width=1000, height=800):\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    i=0\n",
    "    while i<100: # we will acquire 100 frames you may change this if you like\n",
    "        ret_val, img = cam.read()\n",
    "             \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # you can first test it like this (to see if your webcam works)\n",
    "        # then add face detection or face+eye detection and rectangle drawings\n",
    "        cv2.imshow('my webcam', img)\n",
    "        cv2.namedWindow('my webcam',cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('my webcam', width, height)\n",
    "        i+=1\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some outputs from your camera here if you like (screenshots would do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
